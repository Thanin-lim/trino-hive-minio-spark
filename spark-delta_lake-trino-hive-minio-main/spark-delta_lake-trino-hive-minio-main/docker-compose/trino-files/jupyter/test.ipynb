{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a282d680-8896-4397-ad67-b04d9fc64700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions \n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.functions import col, from_json, window, count,trunc,to_timestamp,date_trunc\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "from confluent_kafka import Producer\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "# from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b04c6cc-406f-46a1-b879-b28bc6d43708",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"test-lmwn\")\n",
    "    # s3\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://s3:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"miniopassword\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    # iceberg\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.iceberg.type\", \"hadoop\")\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3a://lmwn/iceberg\")\n",
    "\n",
    "    # kafka\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\"\n",
    "    )\n",
    "\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b461de5-5679-4b56-a2e1-e72a5b3cb5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcUrl = \"jdbc:postgresql://test-postgresql-1:5432/postgres\"\n",
    "connectionProperties = {\n",
    "  \"user\": \"postgres\",\n",
    "  \"password\": \"\",\n",
    "  \"driver\": 'org.postgresql.Driver' # Specify the driver class\n",
    "}\n",
    "# df = spark.read.jdbc(jdbcUrl, \"public.products\", properties=connectionProperties)\n",
    "# products.write.mode(\"overwrite\") .jdbc(jdbcUrl, \"public.products\", properties=connectionProperties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b18617f-8c99-4c25-bb73-802ac3eea60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('order_id', StringType(), True), StructField('order_date', StringType(), True), StructField('user_id', StringType(), True), StructField('product_id', StringType(), True), StructField('quantity', IntegerType(), True), StructField('status', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "order_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "])\n",
    "print(order_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "888c7352-5ca6-4f2f-911f-850682e25015",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.option('header','true').csv('s3a://lmwn/data/orders.csv').alias('o')\n",
    "products = spark.read.option('header','true').csv('s3a://lmwn/data/products.csv').alias('p')\n",
    "orders_pg = spark.read.jdbc(jdbcUrl, \"public.orders\", properties=connectionProperties)\n",
    "products_pg = spark.read.jdbc(jdbcUrl, \"public.products\", properties=connectionProperties)\n",
    "orders_iceberg = spark.table('iceberg.orders')\n",
    "products_iceberg = spark.table('iceberg.products')\n",
    "orders_stream = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:9092\").option(\"subscribe\", \"orders\").option(\"startingOffsets\", \"earliest\").load()\n",
    "order_product = orders.join(products,col('o.product_id')==col('p.product_id'),'left').select( col('order_id'),\n",
    "                                                                                                  col('order_date'),\n",
    "                                                                                                  col('user_id'),\n",
    "                                                                                                  col('o.product_id'),\n",
    "                                                                                                  col('quantity'),\n",
    "                                                                                                  col('status'),\n",
    "                                                                                                  col('product_name'),\n",
    "                                                                                                  col('price'),\n",
    "                                                                                                  col('category'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f29dbe-58a6-4fd1-b513-3ac7ad3f94fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2cb1387-e3d3-42fa-ba67-901ec2151cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_stream = (\n",
    "    orders_stream.select(\"timestamp\", \"value\")\n",
    "      .withColumn(\"value\", from_json(col(\"value\").cast(\"string\"), order_schema)).\n",
    "                select(col('timestamp'),\n",
    "                    date_trunc(\"minute\", col(\"timestamp\")).alias(\"window_start\"),\n",
    "                    col(\"value.product_id\").alias(\"product_id\"),\n",
    "                    col(\"value.order_id\").alias(\"order_id\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dc54003-0a3d-4514-be72-c46bc45eeca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df =  order_stream.withWatermark('timestamp', \"1 minutes\")\\\n",
    "                .groupBy(col(\"product_id\"),window(col('timestamp'),\"1 minutes\"))\\\n",
    "                    .agg(count(col(\"order_id\")).alias(\"order_count\")).select(\n",
    "                        col(\"product_id\"),             \n",
    "                        col(\"window.start\").alias(\"window_start\"),\n",
    "                        col(\"window.end\").alias(\"window_end\"),\n",
    "                        col(\"order_count\")\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4376a5-e3e3-4d01-a4c1-f17e74f36678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418bb358-f783-4f56-b00d-1407bd58f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_product.write.mode(\"overwrite\") .jdbc(jdbcUrl, \"public.order_product\", properties=connectionProperties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ebf82-82fb-44a7-8459-3a6270a243c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_product.writeTo(\"iceberg.order_product\").append() # .create() #.overwritePartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2b6ca93-ccf5-4bef-a097-06f536fa953e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7fdc82fed0d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_stream.writeStream.outputMode(\"append\")\\\n",
    "    .format(\"iceberg\")\\\n",
    "    .option(\"checkpointLocation\", \"s3a://lmwn/checkpoints/orders\")\\\n",
    "    .toTable(\"iceberg.stream.orders_stream\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5860ddb7-7160-48d3-bb0d-bc781abd1214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7fdc82fee010>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.writeStream.outputMode(\"append\")\\\n",
    "    .format(\"iceberg\")\\\n",
    "    .option(\"checkpointLocation\", \"s3a://lmwn/checkpoints/orders\")\\\n",
    "    .toTable(\"iceberg.stream.orders_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f317b919-a5c3-47ca-be0d-bf8f55cfb7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
