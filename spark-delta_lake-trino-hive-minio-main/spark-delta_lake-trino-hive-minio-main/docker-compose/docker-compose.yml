version: "3.4"

services:
  # --- 1. Database ---
  postgresdb:
    image: postgres:13
    hostname: metastore_db
    restart: always
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      POSTGRES_DB: metastore
      PGDATA: /var/lib/postgresql/data 
    volumes:
      - /data/postgres:/var/lib/postgresql
    networks:
      - data-network

  # --- 2. Storage ---
  minio:
    image: 'minio/minio:latest'
    hostname: minio
    container_name: minio
    ports:
      - '9000:9000'
      - '9001:9001'
    volumes:
      - minio-data:/data
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
    command: server --console-address ":9001" /data
    networks:
      - data-network

  # --- 3. Metadata Service ---

  hive-metastore:
    # This is the Hive Metastore service
    image: warvba22/hive-metastore:0.1.0 
    # This is the hive server hostname
    hostname: hive-metastore
    ports:
      # Expose Metastore Thrift service on port 9083.
      - '9083:9083'
    # In case of service/container crash, the container will restart.
    restart: always
    environment:
      # JDBC driver for connecting Hive Metastore to Postgres
      HIVE_METASTORE_DRIVER: org.postgresql.Driver
      
      # JDBC URL for the Postgres DB. Ensure the hostname and port match your Postgres service.
      HIVE_METASTORE_JDBC_URL: jdbc:postgresql://metastore_db:5432/metastore
      
      # Username for connecting to Postgres from Hive. Change this if you modified POSTGRES_USER.
      HIVE_METASTORE_USER: hive
      
      # Password for connecting to Postgres from Hive. Change this if you modified POSTGRES_PASSWORD.
      HIVE_METASTORE_PASSWORD: hive
      
      # Location of the Hive warehouse where Delta Lake data is stored.
      # You can modify this to match the bucket or path in your MinIO setup.
      HIVE_METASTORE_WAREHOUSE_DIR: s3://spark-delta-lake/
      
      # MinIO endpoint. Replace this with your MinIO address and port.
      S3_ENDPOINT: http://minio:9000
      
      # Access key for MinIO. Change this to your actual MinIO access key.
      S3_ACCESS_KEY: minio
      S3_SECRET_KEY: minio123
      
      # Use path-style access. Keep this set to "true" for MinIO compatibility.
      S3_PATH_STYLE_ACCESS: "true"
      
      # The region for S3 storage. For MinIO, leave this blank.
      REGION: ""

      # Following fields are placeholders for integration with other cloud services like Google Cloud and Azure.
      # These can be left blank unless you're using one of those services.
      GOOGLE_CLOUD_KEY_FILE_PATH: ""
      AZURE_ADL_CLIENT_ID: ""
      AZURE_ADL_CREDENTIAL: ""
      AZURE_ADL_REFRESH_URL: ""
      AZURE_ABFS_STORAGE_ACCOUNT: ""
      AZURE_ABFS_ACCESS_KEY: ""
      AZURE_WASB_STORAGE_ACCOUNT: ""
      AZURE_ABFS_OAUTH: ""
      AZURE_ABFS_OAUTH_TOKEN_PROVIDER: ""
      AZURE_ABFS_OAUTH_CLIENT_ID: ""
      AZURE_ABFS_OAUTH_SECRET: ""
      AZURE_ABFS_OAUTH_ENDPOINT: ""
      AZURE_WASB_ACCESS_KEY: ""

      # Define Hive Metastore admin role. You can change the role if needed.
      HIVE_METASTORE_USERS_IN_ADMIN_ROLE: "admin"
    healthcheck:
      # Health check to confirm if the Hive Metastore is running on port 9083.
      test: bash -c "exec 6<> /dev/tcp/localhost/9083"

    networks:
      - data-network

  # --- 4. Query Engine ---
  trino-coordinator:
      image: 'trinodb/trino:400'
      container_name: trino-coordinator
      hostname: trino-coordinator
      restart: always
      ports:
        - '8080:8080'
      volumes:
        - ./trino-files:/etc/trino
        - ./trino-data-coordinator:/data/trino # แยกที่เก็บข้อมูล
      networks:
        - data-network

  trino-worker-1:
      image: 'trinodb/trino:400'
      container_name: trino-worker-1
      restart: always
      volumes:
        - ./trino-config/worker:/etc/trino
        - ./trino-data-worker-1:/data/trino # แยกที่เก็บข้อมูล
      depends_on:
        - trino-coordinator
      networks:
        - data-network

  trino-worker-2:
      image: 'trinodb/trino:400'
      container_name: trino-worker-2
      restart: always
      volumes:
        - ./trino-config/worker:/etc/trino
        - ./trino-data-worker-2:/data/trino # แยกที่เก็บข้อมูล
      depends_on:
        - trino-coordinator
      networks:
        - data-network
  spark-master:
    image: nechayut/spark-aws:1.0.0  
    container_name: spark-master_driver
    hostname: spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "8082:8080"
      - "7077:7077"
    environment:
      - SPARK_NO_DAEMONIZE=true
    volumes:
      - ./spark-apps:/opt/spark-apps
      - ./spark-data:/opt/spark-data
      - ./src:/opt/src
    networks:
      - data-network
# Spark Worker 1
  spark-worker-1:
    image: nechayut/spark-aws:1.0.0  
    container_name: spark-worker-1
    hostname: spark-worker-1
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    ports:
      - "8083:8081" 
    environment:
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_NO_DAEMONIZE=true
      # - SPARK_WORKER_WEBUI_PORT=8081 (ค่า default ไม่ต้องใส่ก็ได้)
    volumes:
      - ./spark-apps:/opt/spark-apps
      - ./src:/opt/src
    networks:
      - data-network

  # Spark Worker 2
  spark-worker-2:
    image: nechayut/spark-aws:1.0.0  
    container_name: spark-worker-2
    hostname: spark-worker-2
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    ports:
      - "8084:8081" 
    environment:
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_NO_DAEMONIZE=true
    volumes:
      - ./spark-apps:/opt/spark-apps
      - ./spark-data:/opt/spark-data
      - ./src:/opt/src
    networks:
      - data-network


  jupyter:
      image:  nechayut/jupyter-spark:1.0.0
      container_name: jupyter
      ports:
        - "8888:8888" # Jupyter UI
        - "4040:4040" # Spark UI
        - "20020:20020" # Spark Driver Port
        - "20021:20021" # Spark BlockManager Port
      environment:
        - JUPYTER_ENABLE_LAB=yes
        # แนะนำให้กำหนดตัวแปรให้ Spark รู้ด้วย (Optional แต่ดี)
        - SPARK_DRIVER_PORT=20020
        - SPARK_BLOCKMANAGER_PORT=20021
      volumes:
        - ./jupyter:/home/jovyan/work
      depends_on:
        - spark-master
      networks:
        - data-network
volumes:
  minio-data:
    driver: local

networks:
  data-network:
    driver: bridge