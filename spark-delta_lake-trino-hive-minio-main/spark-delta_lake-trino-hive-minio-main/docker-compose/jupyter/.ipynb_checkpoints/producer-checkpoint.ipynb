{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e583919-f294-4a1c-af34-5297b16374b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install jupyter_contrib_nbextensions\n",
    "# pip install pyspark\n",
    "# pip install pandas\n",
    "# pip install confluent_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a282d680-8896-4397-ad67-b04d9fc64700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions \n",
    "from pyspark.sql.functions import col\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "from confluent_kafka import Producer\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b461de5-5679-4b56-a2e1-e72a5b3cb5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcUrl = \"jdbc:postgresql://test-postgresql-1:5432/postgres\"\n",
    "connectionProperties = {\n",
    "  \"user\": \"postgres\",\n",
    "  \"password\": \"\",\n",
    "  \"driver\": 'org.postgresql.Driver' # Specify the driver class\n",
    "}\n",
    "# df = spark.read.jdbc(jdbcUrl, \"public.products\", properties=connectionProperties)\n",
    "# products.write.mode(\"overwrite\") .jdbc(jdbcUrl, \"public.products\", properties=connectionProperties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70094c6-7590-4588-896a-568b70b0e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_HOST = \"127.0.0.1:9093\"\n",
    "data = {\n",
    "    \"products\" : pd.read_csv(\"./data/products.csv\"),\n",
    "    # \"orders\"   : pd.read_csv(\"./data/orders.csv\")\n",
    "}\n",
    "\n",
    "admin = AdminClient({ \"bootstrap.servers\" : KAFKA_HOST })\n",
    "\n",
    "for my_topic, df in data.items():\n",
    "\n",
    "    \n",
    "    #### delete topic\n",
    "    print(f\"Deleting topic {my_topic} if exists\")\n",
    "    ops = admin.delete_topics([my_topic], operation_timeout=30)\n",
    "    for topic, f in ops.items():\n",
    "        try:\n",
    "            f.result()\n",
    "            print(f\"Topic {topic} is deleted\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete topic {topic}: {e}\")\n",
    "\n",
    "\n",
    "    \n",
    "    #### recreate topic\n",
    "    print(f\"Creating topic {my_topic}\")\n",
    "    new_topics = [NewTopic(my_topic, num_partitions=3, replication_factor=1)]\n",
    "    ops = admin.create_topics(new_topics)\n",
    "    \n",
    "    for topic, f in ops.items():\n",
    "        try:\n",
    "            f.result()\n",
    "            print(f\"Topic {topic} is created\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create topic {topic}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    # import data to Kafka\n",
    "    producer = Producer({ \"bootstrap.servers\" : KAFKA_HOST })\n",
    "    print(f\"Importing {my_topic} data\")\n",
    "\n",
    "    cnt = 0\n",
    "    batch_size = 5e4\n",
    "    for _, row in df.iterrows():\n",
    "        if cnt % batch_size==0 and cnt!=0:\n",
    "            producer.flush()\n",
    "            print(f\"{cnt} message is imported\")\n",
    "        producer.produce(my_topic, json.dumps(row.to_dict()))\n",
    "        cnt+=1\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b04c6cc-406f-46a1-b879-b28bc6d43708",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"cluster-read-s3\")\n",
    "    # s3\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://s3:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"miniopassword\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    # iceberg\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.iceberg.type\", \"hadoop\")\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3a://lmwn/iceberg\")\n",
    "\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "888c7352-5ca6-4f2f-911f-850682e25015",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.option('header','true').csv('s3a://lmwn/data/orders.csv').alias('o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e10883c-f213-4124-80d6-ecc3e58f646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = spark.read.option('header','true').csv('s3a://lmwn/data/products.csv').alias('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfce150c-ebfc-4c28-b4a1-3bf1efd99b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_product = orders.join(products,col('o.product_id')==col('p.product_id'),'left').select( col('order_id'),\n",
    "                                                                                                  col('order_date'),\n",
    "                                                                                                  col('user_id'),\n",
    "                                                                                                  col('o.product_id'),\n",
    "                                                                                                  col('quantity'),\n",
    "                                                                                                  col('status'),\n",
    "                                                                                                  col('product_name'),\n",
    "                                                                                                  col('price'),\n",
    "                                                                                                  col('category'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "afc98918-42f1-4837-8fad-1abcf5d153a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_product.write.mode('overwrite').csv('./data/order_product.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a348230a-5b3e-4908-8f97-1e8adc042a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "| user_id|    total_revenue|\n",
      "+--------+-----------------+\n",
      "|USER-001|521671.7600000001|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select user_id,\n",
    "        sum(price*quantity) as total_revenue \n",
    "from (\n",
    "        select order_id,product_id,user_id,quantity \n",
    "        from iceberg.orders \n",
    "        where user_id = 'USER-001' and status = 'COMPLETE') orders \n",
    "left join (\n",
    "        select product_id,price \n",
    "        from iceberg.products) prod \n",
    "on orders.product_id = prod.product_id\n",
    "group by user_id;\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "85b4d7f1-0342-4af1-8532-06b4f0e81c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .jdbc(jdbcUrl, \"public.products\", properties=connectionProperties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a24e306-65d0-40d9-9b50-43c6d7e6c3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "products.write.mode(\"overwrite\") .jdbc(jdbcUrl, \"public.products\", properties=connectionProperties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc612dd5-6858-49bd-b493-47caa33dcf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b18617f-8c99-4c25-bb73-802ac3eea60f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
