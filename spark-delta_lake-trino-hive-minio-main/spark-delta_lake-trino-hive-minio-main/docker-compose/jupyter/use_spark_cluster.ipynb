{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04384c40-62b5-4e91-95eb-d857bb02d951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93745f39-075d-4854-8d33-42e795f0eb64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233e4d2a-78bd-41be-98d7-dd19ecc8949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ‡∏î‡∏∂‡∏á IP ‡∏Ç‡∏≠‡∏á Container ‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á (Jupyter)\n",
    "hostname = socket.gethostname()\n",
    "local_ip = socket.gethostbyname(hostname)\n",
    "print(f\"üîπ Jupyter IP: {local_ip}\")\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "MINIO_ENDPOINT = \"http://minio:9000\"\n",
    "HIVE_METASTORE_URI = \"thrift://docker-compose-hive-metastore-1:9083\" # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ä‡∏∑‡πà‡∏≠ Container Hive ‡∏î‡∏µ‡πÜ\n",
    "ACCESS_KEY = \"minio\"\n",
    "SECRET_KEY = \"minio123\"\n",
    "\n",
    "PACKAGES = [\n",
    "    \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3\",\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\"\n",
    "]\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Jupyter-Iceberg-Cluster\")\n",
    "    \n",
    "    # ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Spark Master (‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ service ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô network ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # ‚úÖ ‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ Network Issue\n",
    "    # 1. ‡∏ö‡∏≠‡∏Å Worker ‡∏ß‡πà‡∏≤ Driver ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà IP ‡πÑ‡∏´‡∏ô\n",
    "    .config(\"spark.driver.host\", local_ip) \n",
    "    \n",
    "    # 2. ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ Port ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ß‡πâ‡πÉ‡∏ô Docker Compose\n",
    "    .config(\"spark.driver.port\", \"20020\")\n",
    "    .config(\"spark.blockManager.port\", \"20021\")\n",
    "    \n",
    "    # 3. ‡πÉ‡∏´‡πâ Driver ‡∏ü‡∏±‡∏á‡∏ó‡∏∏‡∏Å Interface ‡πÉ‡∏ô Container ‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    .config(\"spark.jars.packages\", \",\".join(PACKAGES))\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "\n",
    "    # S3 Config\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", ACCESS_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", SECRET_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \n",
    "    # Iceberg Config\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.iceberg.type\", \"hive\")\n",
    "    .config(\"spark.sql.catalog.iceberg.uri\", HIVE_METASTORE_URI)\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3a://warehouse/iceberg_data\")\n",
    "    \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"‚úÖ Spark Session Created with Fixed Ports!\")\n",
    "\n",
    "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö\n",
    "spark.sql(\"SELECT * FROM iceberg.demo_schema.jupyter_users\").show()\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c954069c-65ff-4248-b074-c85a7e61a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ‡∏î‡∏∂‡∏á IP ‡∏Ç‡∏≠‡∏á Container ‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á (Jupyter)\n",
    "hostname = socket.gethostname()\n",
    "local_ip = socket.gethostbyname(hostname)\n",
    "print(f\"üîπ Jupyter IP: {local_ip}\")\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "MINIO_ENDPOINT = \"http://minio:9000\"\n",
    "HIVE_METASTORE_URI = \"thrift://docker-compose-hive-metastore-1:9083\" # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ä‡∏∑‡πà‡∏≠ Container Hive ‡∏î‡∏µ‡πÜ\n",
    "ACCESS_KEY = \"minio\"\n",
    "SECRET_KEY = \"minio123\"\n",
    "\n",
    "PACKAGES = [\n",
    "    \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3\",\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\"\n",
    "]\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Jupyter-Iceberg-Cluster\")\n",
    "    \n",
    "    # ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Spark Master (‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ service ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô network ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # ‚úÖ ‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ Network Issue\n",
    "    # 1. ‡∏ö‡∏≠‡∏Å Worker ‡∏ß‡πà‡∏≤ Driver ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà IP ‡πÑ‡∏´‡∏ô\n",
    "    .config(\"spark.driver.host\", local_ip) \n",
    "    \n",
    "    # 2. ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ Port ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ß‡πâ‡πÉ‡∏ô Docker Compose\n",
    "    .config(\"spark.driver.port\", \"20020\")\n",
    "    .config(\"spark.blockManager.port\", \"20021\")\n",
    "    \n",
    "    # 3. ‡πÉ‡∏´‡πâ Driver ‡∏ü‡∏±‡∏á‡∏ó‡∏∏‡∏Å Interface ‡πÉ‡∏ô Container ‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    .config(\"spark.jars.packages\", \",\".join(PACKAGES))\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "\n",
    "    # S3 Config\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", ACCESS_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", SECRET_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \n",
    "    # Iceberg Config\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.iceberg.type\", \"hive\")\n",
    "    .config(\"spark.sql.catalog.iceberg.uri\", HIVE_METASTORE_URI)\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3a://warehouse/iceberg_data\")\n",
    "    \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"‚úÖ Spark Session Created with Fixed Ports!\")\n",
    "\n",
    "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö\n",
    "spark.sql(\"SELECT * FROM iceberg.demo_schema.jupyter_users\").show()\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095a2de9-55de-4c3b-9d88-7dcd4ec0dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergFix\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ Python 3.11 ‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏´‡∏°\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c96092-5985-463a-9491-7fd1144cc64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM iceberg.demo_schema.jupyter_users \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7bec9db-27b8-4ad0-8d92-9c0eed9bef92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 1: Create Table & Insert Data ---\n",
      "+---+-----+--------+------+\n",
      "| id| name|    role|salary|\n",
      "+---+-----+--------+------+\n",
      "|  1|Alice|Engineer| 50000|\n",
      "|  2|  Bob| Manager| 80000|\n",
      "+---+-----+--------+------+\n",
      "\n",
      "\n",
      "--- STEP 2: MERGE (Upsert) ---\n",
      "Scenario: ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô Bob ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà Charlie\n",
      "+---+-------+--------------+------+\n",
      "| id|   name|          role|salary|\n",
      "+---+-------+--------------+------+\n",
      "|  1|  Alice|      Engineer| 50000|\n",
      "|  2|    Bob|Senior Manager| 90000|\n",
      "|  3|Charlie|      Designer| 45000|\n",
      "+---+-------+--------------+------+\n",
      "\n",
      "\n",
      "--- STEP 3: DELETE ---\n",
      "Scenario: ‡∏•‡∏ö‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏∑‡πà‡∏≠ Alice ‡∏≠‡∏≠‡∏Å\n",
      "+---+-------+--------------+------+\n",
      "| id|   name|          role|salary|\n",
      "+---+-------+--------------+------+\n",
      "|  2|    Bob|Senior Manager| 90000|\n",
      "|  3|Charlie|      Designer| 45000|\n",
      "+---+-------+--------------+------+\n",
      "\n",
      "Final Row Count: 2\n",
      "\n",
      "‚úÖ Session Closed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import socket\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# --- 1. CONFIGURATION (‡∏ä‡∏∏‡∏î‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á) ---\n",
    "hostname = socket.gethostname()\n",
    "local_ip = socket.gethostbyname(hostname)\n",
    "\n",
    "MINIO_ENDPOINT = \"http://minio:9000\"\n",
    "HIVE_METASTORE_URI = \"thrift://docker-compose-hive-metastore-1:9083\"\n",
    "ACCESS_KEY = \"minio\"\n",
    "SECRET_KEY = \"minio123\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Iceberg-CRUD-Demo\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    # Network Fix\n",
    "    .config(\"spark.driver.host\", local_ip)\n",
    "    .config(\"spark.driver.port\", \"20020\")\n",
    "    .config(\"spark.blockManager.port\", \"20021\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "\n",
    "    # Packages & Extensions\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3,org.apache.hadoop:hadoop-aws:3.3.4\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "\n",
    "    # MinIO / S3\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", ACCESS_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", SECRET_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \n",
    "    \n",
    "    # Iceberg Catalog\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.iceberg.type\", \"hive\")\n",
    "    .config(\"spark.sql.catalog.iceberg.uri\", HIVE_METASTORE_URI)\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3a://warehouse/iceberg_data\")\n",
    "    \n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "try:\n",
    "    # ==========================================\n",
    "    # üü¢ 1. CREATE & INSERT (‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)\n",
    "    # ==========================================\n",
    "    print(\"\\n--- STEP 1: Create Table & Insert Data ---\")\n",
    "    \n",
    "    # ‡∏•‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏¥‡πâ‡∏á‡∏Å‡πà‡∏≠‡∏ô (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà\n",
    "    spark.sql(\"DROP TABLE IF EXISTS iceberg.demo_schema.employees\")\n",
    "    \n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS iceberg.demo_schema.employees (\n",
    "            id INT,\n",
    "            name STRING,\n",
    "            role STRING,\n",
    "            salary INT\n",
    "        ) USING iceberg\n",
    "    \"\"\")\n",
    "    \n",
    "    # ‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (Alice ‡πÅ‡∏•‡∏∞ Bob)\n",
    "    spark.sql(\"\"\"\n",
    "        INSERT INTO iceberg.demo_schema.employees VALUES \n",
    "        (1, 'Alice', 'Engineer', 50000),\n",
    "        (2, 'Bob',   'Manager',  80000)\n",
    "    \"\"\")\n",
    "    \n",
    "    spark.sql(\"SELECT * FROM iceberg.demo_schema.employees ORDER BY id\").show()\n",
    "\n",
    "\n",
    "    # ==========================================\n",
    "    # üü† 2. MERGE (Upsert: ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà + ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà)\n",
    "    # ==========================================\n",
    "    print(\"\\n--- STEP 2: MERGE (Upsert) ---\")\n",
    "    print(\"Scenario: ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô Bob ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà Charlie\")\n",
    "\n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Data Frame ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤\n",
    "    new_data = [\n",
    "        (2, 'Bob', 'Senior Manager', 90000),  # Bob (‡∏Ñ‡∏ô‡πÄ‡∏î‡∏¥‡∏° -> ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Role/Salary)\n",
    "        (3, 'Charlie', 'Designer', 45000)     # Charlie (‡∏Ñ‡∏ô‡πÉ‡∏´‡∏°‡πà -> ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ)\n",
    "    ]\n",
    "    df_updates = spark.createDataFrame(new_data, [\"id\", \"name\", \"role\", \"salary\"])\n",
    "    \n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Temp View ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô SQL\n",
    "    df_updates.createOrReplaceTempView(\"incoming_data\")\n",
    "\n",
    "    # ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á MERGE\n",
    "    # ‡∏ñ‡πâ‡∏≤ id ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô -> ‡πÉ‡∏´‡πâ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï (WHEN MATCHED)\n",
    "    # ‡∏ñ‡πâ‡∏≤ id ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠  -> ‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà (WHEN NOT MATCHED)\n",
    "    spark.sql(\"\"\"\n",
    "        MERGE INTO iceberg.demo_schema.employees AS target\n",
    "        USING incoming_data AS source\n",
    "        ON target.id = source.id\n",
    "        WHEN MATCHED THEN \n",
    "            UPDATE SET target.role = source.role, target.salary = source.salary\n",
    "        WHEN NOT MATCHED THEN \n",
    "            INSERT (id, name, role, salary) VALUES (source.id, source.name, source.role, source.salary)\n",
    "    \"\"\")\n",
    "\n",
    "    spark.sql(\"SELECT * FROM iceberg.demo_schema.employees ORDER BY id\").show()\n",
    "\n",
    "\n",
    "    # ==========================================\n",
    "    # üî¥ 3. DELETE (‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç)\n",
    "    # ==========================================\n",
    "    print(\"\\n--- STEP 3: DELETE ---\")\n",
    "    print(\"Scenario: ‡∏•‡∏ö‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏∑‡πà‡∏≠ Alice ‡∏≠‡∏≠‡∏Å\")\n",
    "\n",
    "    spark.sql(\"\"\"\n",
    "        DELETE FROM iceberg.demo_schema.employees \n",
    "        WHERE name = 'Alice'\n",
    "    \"\"\")\n",
    "\n",
    "    # ‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡∏•‡∏∑‡∏≠ Bob ‡πÅ‡∏•‡∏∞ Charlie)\n",
    "    final_df = spark.sql(\"SELECT * FROM iceberg.demo_schema.employees ORDER BY id\")\n",
    "    final_df.show()\n",
    "\n",
    "    print(f\"Final Row Count: {final_df.count()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    # spark.stop()\n",
    "    print(\"\\n‚úÖ Session Closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9398c9a-933d-4cd7-bad4-0dc1f4e82125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+\n",
      "| id|   username|          created_at|\n",
      "+---+-----------+--------------------+\n",
      "|  4|test_insert|2026-01-31 09:06:...|\n",
      "|  1|       test|2026-01-31 08:51:...|\n",
      "|  2|Spark_Ninja|2026-01-31 08:51:...|\n",
      "|  3|Hive_Master|2026-01-31 08:51:...|\n",
      "+---+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM iceberg.demo_schema.jupyter_users \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f5b6cc-67c8-4c52-aa57-9a713694a275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4024121d-058d-44b0-bb27-77592a05d999",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (Spark)",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
