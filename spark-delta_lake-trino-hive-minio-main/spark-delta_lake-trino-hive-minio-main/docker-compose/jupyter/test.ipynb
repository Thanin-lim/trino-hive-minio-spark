{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a282d680-8896-4397-ad67-b04d9fc64700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions \n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.functions import col, from_json, window, count,trunc,to_timestamp,date_trunc\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "# from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b04c6cc-406f-46a1-b879-b28bc6d43708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-16a07e74-22bf-4d0c-92f1-71cdc89e7805;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 389ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-16a07e74-22bf-4d0c-92f1-71cdc89e7805\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/8ms)\n",
      "26/02/01 07:33:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/01 07:33:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MINIO_ENDPOINT = \"http://minio:9000\"\n",
    "HIVE_METASTORE_URI = \"thrift://docker-compose-hive-metastore-1:9083\" \n",
    "\n",
    "# Credentials (‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö docker-compose)\n",
    "ACCESS_KEY = \"minio\"\n",
    "SECRET_KEY = \"minio123\"\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"test-lmwn\")\n",
    "\n",
    "    # s3\n",
    "    # .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://s3:9000\")\n",
    "    # .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    # .config(\"spark.hadoop.fs.s3a.secret.key\", \"miniopassword\")\n",
    "    # .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    # .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    # .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "    # --- S3 (MinIO) Config ---\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", ACCESS_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", SECRET_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # # iceberg\n",
    "    # .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    # .config(\"spark.sql.catalog.iceberg.type\", \"hive\")\n",
    "    # .config(\"spark.sql.catalog.iceberg.uri\", HIVE_METASTORE_URI)\n",
    "    # .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3a://warehouse/iceberg_data\")\n",
    "\n",
    "   # --- Iceberg Catalog Config (‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏° Hive) ---\n",
    "    # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ iceberg ‡πÄ‡∏õ‡πá‡∏ô hive ‡πÉ‡∏´‡πâ‡∏´‡∏°‡∏î\n",
    "    .config(\"spark.sql.catalog.hive\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.hive.type\", \"hive\")\n",
    "    .config(\"spark.sql.catalog.hive.uri\", HIVE_METASTORE_URI)\n",
    "    .config(\"spark.sql.catalog.hive.warehouse\", \"s3a://warehouse/users2\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # kafka\n",
    "    \n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\"\n",
    "    )\n",
    "\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d57f5ac3-1874-4645-916f-8fab2fb1304d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipython-sql\n",
      "  Downloading ipython_sql-0.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: sqlalchemy in /opt/conda/lib/python3.11/site-packages (2.0.22)\n",
      "Requirement already satisfied: trino[sqlalchemy] in /opt/conda/lib/python3.11/site-packages (0.336.0)\n",
      "Collecting prettytable (from ipython-sql)\n",
      "  Downloading prettytable-3.17.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.11/site-packages (from ipython-sql) (8.16.1)\n",
      "Collecting sqlparse (from ipython-sql)\n",
      "  Downloading sqlparse-0.5.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from ipython-sql) (1.16.0)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.11/site-packages (from ipython-sql) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy) (4.8.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy) (3.0.0)\n",
      "Requirement already satisfied: lz4 in /opt/conda/lib/python3.11/site-packages (from trino[sqlalchemy]) (4.3.2)\n",
      "Requirement already satisfied: orjson>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from trino[sqlalchemy]) (3.11.6)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.11/site-packages (from trino[sqlalchemy]) (2.8.2)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.11/site-packages (from trino[sqlalchemy]) (2023.3.post1)\n",
      "Requirement already satisfied: requests>=2.32.4 in /opt/conda/lib/python3.11/site-packages (from trino[sqlalchemy]) (2.32.5)\n",
      "Requirement already satisfied: tzlocal in /opt/conda/lib/python3.11/site-packages (from trino[sqlalchemy]) (5.3.1)\n",
      "Requirement already satisfied: zstandard in /opt/conda/lib/python3.11/site-packages (from trino[sqlalchemy]) (0.21.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.4->trino[sqlalchemy]) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.4->trino[sqlalchemy]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.4->trino[sqlalchemy]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.4->trino[sqlalchemy]) (2023.7.22)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.11/site-packages (from ipython->ipython-sql) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from ipython->ipython-sql) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.11/site-packages (from ipython->ipython-sql) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.11/site-packages (from ipython->ipython-sql) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.11/site-packages (from ipython->ipython-sql) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.11/site-packages (from ipython->ipython-sql) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from ipython->ipython-sql) (2.16.1)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.11/site-packages (from ipython->ipython-sql) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.11/site-packages (from ipython->ipython-sql) (5.11.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.11/site-packages (from ipython->ipython-sql) (4.8.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->ipython-sql) (0.2.8)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.11/site-packages (from jedi>=0.16->ipython->ipython-sql) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.11/site-packages (from pexpect>4.3->ipython->ipython-sql) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->ipython-sql) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->ipython-sql) (2.4.0)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->ipython-sql) (0.2.2)\n",
      "Downloading ipython_sql-0.5.0-py3-none-any.whl (20 kB)\n",
      "Downloading prettytable-3.17.0-py3-none-any.whl (34 kB)\n",
      "Downloading sqlparse-0.5.5-py3-none-any.whl (46 kB)\n",
      "Installing collected packages: sqlparse, prettytable, ipython-sql\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3/3\u001b[0m [ipython-sql]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ipython-sql-0.5.0 prettytable-3.17.0 sqlparse-0.5.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipython-sql sqlalchemy trino[sqlalchemy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "322f4026-cf0c-4042-98a0-00c862b947e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trino in /opt/conda/lib/python3.11/site-packages (0.336.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: lz4 in /opt/conda/lib/python3.11/site-packages (from trino) (4.3.2)\n",
      "Requirement already satisfied: orjson>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from trino) (3.11.6)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.11/site-packages (from trino) (2.8.2)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.11/site-packages (from trino) (2023.3.post1)\n",
      "Requirement already satisfied: requests>=2.32.4 in /opt/conda/lib/python3.11/site-packages (from trino) (2.32.5)\n",
      "Requirement already satisfied: tzlocal in /opt/conda/lib/python3.11/site-packages (from trino) (5.3.1)\n",
      "Requirement already satisfied: zstandard in /opt/conda/lib/python3.11/site-packages (from trino) (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil->trino) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.4->trino) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.4->trino) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.4->trino) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.4->trino) (2023.7.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install trino pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "71f24488-259c-4c99-bc17-d5d9652cc340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "# 1. ‡πÇ‡∏´‡∏•‡∏î Extension\n",
    "%load_ext sql\n",
    "\n",
    "# 2. ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ (‡∏≠‡∏¢‡πà‡∏≤‡∏•‡∏∑‡∏°‡πÄ‡∏ä‡πá‡∏Ñ Host ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô localhost ‡∏´‡∏£‡∏∑‡∏≠ docker-name)\n",
    "# ‡∏Å‡∏£‡∏ì‡∏µ‡∏£‡∏±‡∏ô Jupyter ‡πÉ‡∏ô Docker ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô:\n",
    "%sql trino://admin@docker-compose-trino-coordinator-1:8080/iceberg/test_db\n",
    "\n",
    "# ‡∏Å‡∏£‡∏ì‡∏µ‡∏£‡∏±‡∏ô Jupyter ‡∏ö‡∏ô Windows/Mac (‡∏Ç‡πâ‡∏≤‡∏á‡∏ô‡∏≠‡∏Å):\n",
    "# %sql trino://admin@localhost:8080/iceberg/test_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f6a09482-08f1-45d7-a904-876f20964ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n",
      " * trino://admin@docker-compose-trino-coordinator-1:8080/iceberg/test_db\n",
      "Done.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'PLAIN_COLUMNS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msql\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrino://admin@docker-compose-trino-coordinator-1:8080/iceberg/test_db\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 3. ‡∏£‡∏±‡∏ô Query (‡∏î‡∏∂‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà Spark ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ Spark ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á 'jupyter_users' ‡πÉ‡∏ô 'demo_schema' ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß‡∏ô‡∏∞\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msql\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSELECT count(*) FROM iceberg.demo_schema.jupyter_users\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2432\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2430\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2431\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2432\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2434\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2435\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2436\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sql/magic.py:219\u001b[0m, in \u001b[0;36mSqlMagic.execute\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43msql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparsed\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msql\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    222\u001b[0m         result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;66;03m# Instead of returning values, set variables directly in the\u001b[39;00m\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;66;03m# user's namespace. Variable names given by column names\u001b[39;00m\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautopandas:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sql/run.py:374\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(conn, sql, config, user_namespace)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m config\u001b[38;5;241m.\u001b[39mfeedback:\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;28mprint\u001b[39m(interpret_rowcount(result\u001b[38;5;241m.\u001b[39mrowcount))\n\u001b[0;32m--> 374\u001b[0m resultset \u001b[38;5;241m=\u001b[39m \u001b[43mResultSet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mautopandas:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resultset\u001b[38;5;241m.\u001b[39mDataFrame()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sql/run.py:116\u001b[0m, in \u001b[0;36mResultSet.__init__\u001b[0;34m(self, sqlaproxy, config)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28mlist\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlaproxy\u001b[38;5;241m.\u001b[39mfetchall())\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfield_names \u001b[38;5;241m=\u001b[39m unduplicate_field_names(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys)\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretty \u001b[38;5;241m=\u001b[39m PrettyTable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfield_names, style\u001b[38;5;241m=\u001b[39m\u001b[43mprettytable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstyle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28mlist\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, [])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'PLAIN_COLUMNS'"
     ]
    }
   ],
   "source": [
    "# 1. ‡πÇ‡∏´‡∏•‡∏î Extension\n",
    "%load_ext sql\n",
    "\n",
    "# --- üõ†Ô∏è FIX: ‡πÅ‡∏Å‡πâ Error 'DEFAULT' Style ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ---\n",
    "%config SqlMagic.style = 'PLAIN_COLUMNS'\n",
    "# ---------------------------------------------\n",
    "\n",
    "# 2. ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Trino (‡∏ñ‡πâ‡∏≤ Jupyter ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Docker Network ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)\n",
    "%sql trino://admin@docker-compose-trino-coordinator-1:8080/iceberg/test_db\n",
    "\n",
    "# 3. ‡∏£‡∏±‡∏ô Query (‡∏î‡∏∂‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà Spark ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ)\n",
    "# ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ Spark ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á 'jupyter_users' ‡πÉ‡∏ô 'demo_schema' ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß‡∏ô‡∏∞\n",
    "%sql SELECT count(*) FROM iceberg.demo_schema.jupyter_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2dd435d2-2497-4f25-9d97-026d15d4cde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Querying data...\n",
      "‚ùå Error: (trino.exceptions.TrinoUserError) TrinoUserError(type=USER_ERROR, name=SYNTAX_ERROR, message=\"line 1:11: mismatched input 'total_count'. Expecting: ',', 'EXCEPT', 'FETCH', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LIMIT', 'OFFSET', 'ORDER', 'UNION', 'WHERE', 'WINDOW', <EOF>\", query_id=20260131_095338_00035_mnivk)\n",
      "[SQL: SELECT *  total_count FROM iceberg.demo_schema.jupyter_users]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Connection Engine\n",
    "# ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô host ‡πÄ‡∏õ‡πá‡∏ô localhost ‡∏ñ‡πâ‡∏≤ Jupyter ‡∏£‡∏±‡∏ô‡∏ô‡∏≠‡∏Å Docker\n",
    "engine = create_engine('trino://admin@docker-compose-trino-coordinator-1:8080/iceberg/test_db')\n",
    "\n",
    "# 2. ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô SQL\n",
    "sql = \"SELECT *  total_count FROM iceberg.demo_schema.jupyter_users\"\n",
    "\n",
    "# 3. ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "try:\n",
    "    print(\"‚è≥ Querying data...\")\n",
    "    df = pd.read_sql(sql, engine)\n",
    "    print(\"‚úÖ Result:\")\n",
    "    print(df)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "finally:\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a1a2ced3-787c-4a33-be17-bf7ca6fc63d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Sending Query to Trino: \n",
      "select * from iceberg.demo_schema.jupyter_users\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_950/187465301.py:25: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(sql_query, conn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Hive_Master</td>\n",
       "      <td>2026-01-31 08:51:08.506066+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>test_insert</td>\n",
       "      <td>2026-01-31 09:06:22.009201+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>2026-01-31 08:51:08.506059+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Spark_Ninja</td>\n",
       "      <td>2026-01-31 08:51:08.506065+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     username                       created_at\n",
       "0   3  Hive_Master 2026-01-31 08:51:08.506066+00:00\n",
       "1   4  test_insert 2026-01-31 09:06:22.009201+00:00\n",
       "2   1         test 2026-01-31 08:51:08.506059+00:00\n",
       "3   2  Spark_Ninja 2026-01-31 08:51:08.506065+00:00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from trino.dbapi import connect\n",
    "\n",
    "# --- 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Connection ---\n",
    "# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏™‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå‡πÑ‡∏õ‡∏´‡∏≤ Trino Server ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà\n",
    "conn = connect(\n",
    "    host=\"docker-compose-trino-coordinator-1\",        # ‡∏´‡∏£‡∏∑‡∏≠ IP ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á Server\n",
    "    port=8080,               # Port ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Trino\n",
    "    user=\"admin\",            # User (‡∏õ‡∏Å‡∏ï‡∏¥ Trino ‡πÑ‡∏°‡πà‡πÄ‡∏ä‡πá‡∏Ñ Password ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏ß‡πâ)\n",
    "    catalog=\"iceberg\",       # Catalog ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏ß‡πâ\n",
    "    schema=\"test_db\",        # Schema ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (‡πÉ‡∏™‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà‡∏Å‡πá‡πÑ‡∏î‡πâ)\n",
    ")\n",
    "\n",
    "# --- 2. ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô SQL ---\n",
    "# ‡πÉ‡∏ä‡πâ SQL ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ (ANSI SQL)\n",
    "sql_query = \"\"\"\n",
    "select * from iceberg.demo_schema.jupyter_users\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(f\"üöÄ Sending Query to Trino: {sql_query}\")\n",
    "\n",
    "# --- 3. ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏™‡πà Pandas DataFrame ---\n",
    "# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ô‡∏µ‡πâ‡∏á‡πà‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πà‡∏≠\n",
    "df = pd.read_sql(sql_query, conn)\n",
    "display(df)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb481479-f09e-4a32-9a2d-41eded3643dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix applied! You can run SQL now.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8585/1258397917.py:4: DeprecationWarning: the 'DEFAULT' constant is deprecated, use the 'TableStyle' enum instead\n",
      "  if not hasattr(prettytable, 'DEFAULT'):\n"
     ]
    }
   ],
   "source": [
    "import prettytable\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ DEFAULT ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏¢‡∏±‡∏î‡∏Ñ‡πà‡∏≤‡∏°‡∏±‡πà‡∏ß‡πÜ (‡πÅ‡∏ï‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ) ‡πÉ‡∏™‡πà‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ\n",
    "if not hasattr(prettytable, 'DEFAULT'):\n",
    "    prettytable.DEFAULT = prettytable.MSWORD_FRIENDLY\n",
    "    \n",
    "print(\"Fix applied! You can run SQL now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8dcf56d-bc2e-4481-8a07-6f4b93be34b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping ipython-sql as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jupysql in /opt/conda/lib/python3.11/site-packages (0.11.1)\n",
      "Requirement already satisfied: prettytable>=3.12.0 in /opt/conda/lib/python3.11/site-packages (from jupysql) (3.17.0)\n",
      "Requirement already satisfied: sqlalchemy in /opt/conda/lib/python3.11/site-packages (from jupysql) (2.0.22)\n",
      "Requirement already satisfied: sqlparse in /opt/conda/lib/python3.11/site-packages (from jupysql) (0.5.5)\n",
      "Requirement already satisfied: ipython-genutils>=0.1.0 in /opt/conda/lib/python3.11/site-packages (from jupysql) (0.2.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from jupysql) (3.1.2)\n",
      "Requirement already satisfied: sqlglot>=11.3.7 in /opt/conda/lib/python3.11/site-packages (from jupysql) (28.7.0)\n",
      "Requirement already satisfied: jupysql-plugin>=0.4.2 in /opt/conda/lib/python3.11/site-packages (from jupysql) (0.4.5)\n",
      "Requirement already satisfied: ploomber-core>=0.2.7 in /opt/conda/lib/python3.11/site-packages (from jupysql) (0.2.27)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from ploomber-core>=0.2.7->jupysql) (6.0.1)\n",
      "Requirement already satisfied: posthog>=3.0 in /opt/conda/lib/python3.11/site-packages (from ploomber-core>=0.2.7->jupysql) (7.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.7 in /opt/conda/lib/python3.11/site-packages (from posthog>=3.0->ploomber-core>=0.2.7->jupysql) (2.32.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from posthog>=3.0->ploomber-core>=0.2.7->jupysql) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /opt/conda/lib/python3.11/site-packages (from posthog>=3.0->ploomber-core>=0.2.7->jupysql) (2.8.2)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from posthog>=3.0->ploomber-core>=0.2.7->jupysql) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from posthog>=3.0->ploomber-core>=0.2.7->jupysql) (1.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.11/site-packages (from posthog>=3.0->ploomber-core>=0.2.7->jupysql) (4.8.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.7->posthog>=3.0->ploomber-core>=0.2.7->jupysql) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.7->posthog>=3.0->ploomber-core>=0.2.7->jupysql) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.7->posthog>=3.0->ploomber-core>=0.2.7->jupysql) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.7->posthog>=3.0->ploomber-core>=0.2.7->jupysql) (2023.7.22)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prettytable>=3.12.0->jupysql) (0.2.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->jupysql) (2.1.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy->jupysql) (3.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y ipython-sql\n",
    "!pip install jupysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ce3f93c-519f-4e26-b66c-5d3dd23028aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting grpcio>=1.48.1\n",
      "  Downloading grpcio-1.76.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting google-auth\n",
      "  Downloading google_auth-2.48.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting typing-extensions~=4.12 (from grpcio>=1.48.1)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: cryptography>=38.0.3 in /opt/conda/lib/python3.11/site-packages (from google-auth) (41.0.4)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth)\n",
      "  Downloading pyasn1-0.6.2-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.11/site-packages (from cryptography>=38.0.3->google-auth) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=38.0.3->google-auth) (2.21)\n",
      "Downloading grpcio-1.76.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading google_auth-2.48.0-py3-none-any.whl (236 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.2-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Installing collected packages: typing-extensions, pyasn1, rsa, pyasn1-modules, grpcio, google-auth\n",
      "\u001b[2K  Attempting uninstall: typing-extensions\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.8.0\n",
      "\u001b[2K    Uninstalling typing_extensions-4.8.0:\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.8.0[32m0/6\u001b[0m [typing-extensions]\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6/6\u001b[0m [google-auth]\u001b[0m [google-auth]es]\n",
      "\u001b[1A\u001b[2KSuccessfully installed google-auth-2.48.0 grpcio-1.76.0 pyasn1-0.6.2 pyasn1-modules-0.4.2 rsa-4.9.1 typing-extensions-4.15.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade \"grpcio>=1.48.1\" google-auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdd60097-70b6-4523-b3ef-42d5cdc8a918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"None\">Connecting to &#x27;trino://admin@docker-compose-trino-coordinator-1:8080/iceberg/test_db&#x27;</span>"
      ],
      "text/plain": [
       "Connecting to 'trino://admin@docker-compose-trino-coordinator-1:8080/iceberg/test_db'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"None\">Running query in &#x27;trino://admin@docker-compose-trino-coordinator-1:8080/iceberg/test_db&#x27;</span>"
      ],
      "text/plain": [
       "Running query in 'trino://admin@docker-compose-trino-coordinator-1:8080/iceberg/test_db'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>id</th>\n",
       "            <th>username</th>\n",
       "            <th>created_at</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>2</td>\n",
       "            <td>Spark_Ninja</td>\n",
       "            <td>2026-01-31 08:51:08.506065+00:00</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3</td>\n",
       "            <td>Hive_Master</td>\n",
       "            <td>2026-01-31 08:51:08.506066+00:00</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1</td>\n",
       "            <td>test</td>\n",
       "            <td>2026-01-31 08:51:08.506059+00:00</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4</td>\n",
       "            <td>test_insert</td>\n",
       "            <td>2026-01-31 09:06:22.009201+00:00</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+----+-------------+----------------------------------+\n",
       "| id |   username  |            created_at            |\n",
       "+----+-------------+----------------------------------+\n",
       "| 2  | Spark_Ninja | 2026-01-31 08:51:08.506065+00:00 |\n",
       "| 3  | Hive_Master | 2026-01-31 08:51:08.506066+00:00 |\n",
       "| 1  |     test    | 2026-01-31 08:51:08.506059+00:00 |\n",
       "| 4  | test_insert | 2026-01-31 09:06:22.009201+00:00 |\n",
       "+----+-------------+----------------------------------+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext sql\n",
    "%sql trino://admin@docker-compose-trino-coordinator-1:8080/iceberg/test_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2961816-b27d-44bf-89b6-8ef2f634425b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"None\">Running query in &#x27;trino://admin@docker-compose-trino-coordinator-1:8080/iceberg/test_db&#x27;</span>"
      ],
      "text/plain": [
       "Running query in 'trino://admin@docker-compose-trino-coordinator-1:8080/iceberg/test_db'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>id</th>\n",
       "            <th>username</th>\n",
       "            <th>created_at</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>3</td>\n",
       "            <td>Hive_Master</td>\n",
       "            <td>2026-01-31 08:51:08.506066+00:00</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4</td>\n",
       "            <td>test_insert</td>\n",
       "            <td>2026-01-31 09:06:22.009201+00:00</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2</td>\n",
       "            <td>Spark_Ninja</td>\n",
       "            <td>2026-01-31 08:51:08.506065+00:00</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1</td>\n",
       "            <td>test</td>\n",
       "            <td>2026-01-31 08:51:08.506059+00:00</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+----+-------------+----------------------------------+\n",
       "| id |   username  |            created_at            |\n",
       "+----+-------------+----------------------------------+\n",
       "| 3  | Hive_Master | 2026-01-31 08:51:08.506066+00:00 |\n",
       "| 4  | test_insert | 2026-01-31 09:06:22.009201+00:00 |\n",
       "| 2  | Spark_Ninja | 2026-01-31 08:51:08.506065+00:00 |\n",
       "| 1  |     test    | 2026-01-31 08:51:08.506059+00:00 |\n",
       "+----+-------------+----------------------------------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT * FROM iceberg.demo_schema.jupyter_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a10390-8790-4ee5-a41a-3f8c37beaafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏£‡∏±‡∏ô Jupyter ‡∏ö‡∏ô Windows/Mac (‡∏ô‡∏≠‡∏Å Docker) ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ 'localhost'\n",
    "# ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏£‡∏±‡∏ô Jupyter ‡πÉ‡∏ô Docker Network ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ Service ‡πÄ‡∏ä‡πà‡∏ô 'minio', 'hive-metastore'\n",
    "MINIO_ENDPOINT = \"http://minio:9000\"\n",
    "HIVE_METASTORE_URI = \"thrift://docker-compose-hive-metastore-1:9083\" \n",
    "\n",
    "# Credentials (‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö docker-compose)\n",
    "ACCESS_KEY = \"minio\"\n",
    "SECRET_KEY = \"minio123\"\n",
    "\n",
    "# Packages ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô (Spark 3.5 + Iceberg + AWS S3)\n",
    "PACKAGES = [\n",
    "    \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3\",\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\" # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ Kafka\n",
    "]\n",
    "\n",
    "# --- 2. BUILD SPARK SESSION ---\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Jupyter-Iceberg-Hive\")\n",
    "    .master(\"local[*]\") # ‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö Local ‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å CPU Core\n",
    "    \n",
    "    # Download Packages\n",
    "    .config(\"spark.jars.packages\", \",\".join(PACKAGES))\n",
    "    \n",
    "    # Extensions (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Update/Delete/Merge ‡πÉ‡∏ô Iceberg)\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "\n",
    "    # --- S3 (MinIO) Config ---\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", ACCESS_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", SECRET_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    # Tuning S3\n",
    "    .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \n",
    "\n",
    "    # --- Iceberg Catalog Config (‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏° Hive) ---\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.iceberg.type\", \"hive\")\n",
    "    .config(\"spark.sql.catalog.iceberg.uri\", HIVE_METASTORE_URI)\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3a://warehouse/iceberg_data\")\n",
    "    \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# ‡∏•‡∏î Log ‡∏Ç‡∏¢‡∏∞ ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà Error/Warn\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"‚úÖ Spark Session Created! Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b461de5-5679-4b56-a2e1-e72a5b3cb5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Operation Failed: An error occurred while calling o49.sql.\n",
      ": org.apache.spark.SparkException: [INTERNAL_ERROR] Undefined error message parameter for error class: '_LEGACY_ERROR_TEMP_1055'. Parameters: Map(database -> iceberg.demo_schema)\n",
      "\tat org.apache.spark.SparkException$.internalError(SparkException.scala:92)\n",
      "\tat org.apache.spark.SparkException$.internalError(SparkException.scala:96)\n",
      "\tat org.apache.spark.ErrorClassesJsonReader.getErrorMessage(ErrorClassesJSONReader.scala:56)\n",
      "\tat org.apache.spark.SparkThrowableHelper$.getMessage(SparkThrowableHelper.scala:53)\n",
      "\tat org.apache.spark.SparkThrowableHelper$.getMessage(SparkThrowableHelper.scala:40)\n",
      "\tat org.apache.spark.sql.AnalysisException.<init>(AnalysisException.scala:47)\n",
      "\tat org.apache.spark.sql.AnalysisException.<init>(AnalysisException.scala:70)\n",
      "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.invalidDatabaseNameError(QueryCompilationErrors.scala:876)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$DatabaseNameInSessionCatalog$.unapply(ResolveSessionCatalog.scala:632)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:52)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:52)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:46)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:226)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:222)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Database (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ)\n",
    "    spark.sql(\"CREATE NAMESPACE IF NOT EXISTS iceberg.demo_schema\")\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS hive.demo_schema.jupyter_users (\n",
    "            id INT,\n",
    "            username STRING,\n",
    "            created_at TIMESTAMP\n",
    "        )\n",
    "        USING iceberg\n",
    "    \"\"\")\n",
    "\n",
    "    # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Mock Data (DataFrame)\n",
    "    from datetime import datetime\n",
    "    data = [\n",
    "        (4, \"test_insert\", datetime.now()),\n",
    "  \n",
    "    ]\n",
    "    df = spark.createDataFrame(data, [\"id\", \"username\", \"created_at\"])\n",
    "\n",
    "    # 4. ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á (Append)\n",
    "    print(\"Writing data to Iceberg/MinIO...\")\n",
    "    df.writeTo(\"iceberg.demo_schema.jupyter_users\").append()\n",
    "\n",
    "    # 5. ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏î‡∏π\n",
    "    print(\"--- üìä Result Query ---\")\n",
    "    result = spark.sql(\"SELECT * FROM hive.demo_schema.jupyter_users ORDER BY id DESC\")\n",
    "    result.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Operation Failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b8adafe-6d97-483f-9a4d-299de5fc21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcUrl = \"jdbc:postgresql://docker-compose-postgresdb-1:5432/postgres\"\n",
    "connectionProperties = {\n",
    "  \"user\": \"postgres\",\n",
    "  \"password\": \"\",\n",
    "  \"driver\": 'org.postgresql.Driver' # Specify the driver class\n",
    "}\n",
    "# df = spark.read.jdbc(jdbcUrl, \"public.products\", properties=connectionProperties)\n",
    "# products.write.mode(\"overwrite\") .jdbc(jdbcUrl, \"public.products\", properties=connectionProperties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c76639a6-c031-4caa-8adb-c468673f8f76",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[REQUIRES_SINGLE_PART_NAMESPACE] spark_catalog requires a single-part namespace, but got `iceberg`.`demo_schema`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupdate iceberg.demo_schema.jupyter_users set username =\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m where username=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mData_Wizard\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [REQUIRES_SINGLE_PART_NAMESPACE] spark_catalog requires a single-part namespace, but got `iceberg`.`demo_schema`."
     ]
    }
   ],
   "source": [
    "spark.sql(\"update iceberg.demo_schema.jupyter_users set username ='test' where username='Data_Wizard' \").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b18617f-8c99-4c25-bb73-802ac3eea60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('order_id', StringType(), True), StructField('order_date', StringType(), True), StructField('user_id', StringType(), True), StructField('product_id', StringType(), True), StructField('quantity', IntegerType(), True), StructField('status', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "order_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "])\n",
    "print(order_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "888c7352-5ca6-4f2f-911f-850682e25015",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.option('header','true').csv('s3a://lmwn/data/orders.csv').alias('o')\n",
    "products = spark.read.option('header','true').csv('s3a://lmwn/data/products.csv').alias('p')\n",
    "orders_pg = spark.read.jdbc(jdbcUrl, \"public.orders\", properties=connectionProperties)\n",
    "products_pg = spark.read.jdbc(jdbcUrl, \"public.products\", properties=connectionProperties)\n",
    "orders_iceberg = spark.table('iceberg.orders')\n",
    "products_iceberg = spark.table('iceberg.products')\n",
    "orders_stream = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:9092\").option(\"subscribe\", \"orders\").option(\"startingOffsets\", \"earliest\").load()\n",
    "order_product = orders.join(products,col('o.product_id')==col('p.product_id'),'left').select( col('order_id'),\n",
    "                                                                                                  col('order_date'),\n",
    "                                                                                                  col('user_id'),\n",
    "                                                                                                  col('o.product_id'),\n",
    "                                                                                                  col('quantity'),\n",
    "                                                                                                  col('status'),\n",
    "                                                                                                  col('product_name'),\n",
    "                                                                                                  col('price'),\n",
    "                                                                                                  col('category'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f29dbe-58a6-4fd1-b513-3ac7ad3f94fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2cb1387-e3d3-42fa-ba67-901ec2151cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_stream = (\n",
    "    orders_stream.select(\"timestamp\", \"value\")\n",
    "      .withColumn(\"value\", from_json(col(\"value\").cast(\"string\"), order_schema)).\n",
    "                select(col('timestamp'),\n",
    "                    date_trunc(\"minute\", col(\"timestamp\")).alias(\"window_start\"),\n",
    "                    col(\"value.product_id\").alias(\"product_id\"),\n",
    "                    col(\"value.order_id\").alias(\"order_id\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dc54003-0a3d-4514-be72-c46bc45eeca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df =  order_stream.withWatermark('timestamp', \"1 minutes\")\\\n",
    "                .groupBy(col(\"product_id\"),window(col('timestamp'),\"1 minutes\"))\\\n",
    "                    .agg(count(col(\"order_id\")).alias(\"order_count\")).select(\n",
    "                        col(\"product_id\"),             \n",
    "                        col(\"window.start\").alias(\"window_start\"),\n",
    "                        col(\"window.end\").alias(\"window_end\"),\n",
    "                        col(\"order_count\")\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4376a5-e3e3-4d01-a4c1-f17e74f36678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418bb358-f783-4f56-b00d-1407bd58f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_product.write.mode(\"overwrite\") .jdbc(jdbcUrl, \"public.order_product\", properties=connectionProperties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ebf82-82fb-44a7-8459-3a6270a243c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_product.writeTo(\"iceberg.order_product\").append() # .create() #.overwritePartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2b6ca93-ccf5-4bef-a097-06f536fa953e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7fdc82fed0d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_stream.writeStream.outputMode(\"append\")\\\n",
    "    .format(\"iceberg\")\\\n",
    "    .option(\"checkpointLocation\", \"s3a://lmwn/checkpoints/orders\")\\\n",
    "    .toTable(\"iceberg.stream.orders_stream\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5860ddb7-7160-48d3-bb0d-bc781abd1214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7fdc82fee010>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.writeStream.outputMode(\"append\")\\\n",
    "    .format(\"iceberg\")\\\n",
    "    .option(\"checkpointLocation\", \"s3a://lmwn/checkpoints/orders\")\\\n",
    "    .toTable(\"iceberg.stream.orders_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f317b919-a5c3-47ca-be0d-bf8f55cfb7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
